The latest version of the code I pushed out this morning includes the changes I have been describing in recent posts that utilize HSL instead of RGB to provide unique mapping between color and sound. This means that each color in the spectrum corresponds to a different pitch. I recently read a paper, titled "Non-verbal Mapping Between Sound and Color - Mapping Derived from Colored Hearing Synesthetes and Its Applications" that gave me the idea to associate the lightness of a color (e.g. how light the color is) with the height of the note. In other words, a dark blue might be a low C, a light blue might be a high C. This latest version of the code implements just that, providing a range of 5 different scales to choose from (as well as a slider for controlling how high or low to start). I'm still tinkering with finding the most appropriate scales for the project, but it will remain user-changeable. The paper also suggests mapping saturation with loudness, but this has not yet been entirely implemented and I'm not sure yet whether I will implement it. This latest version of the code has been pushed to the Chrome Packaged Ap on the Web Store and will be live through my website at some point soon. Getting very close to a final edition.

"Non-verbal Mapping Between Sound and Color - Mapping Derived from Colored Hearing Synesthetes and Its Applications" - http://www-im.dwc.doshisha.ac.jp/~wake-lab/member/swake/pdf/NonVerbal-ICEC05.pdf
